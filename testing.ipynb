{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images between -1 and 1, size 28x28\n",
    "dataset = MNIST(root='./data', download=True, transform=Compose([Resize((28, 28)), ToTensor(), Lambda(lambda x: (x - 1/2) * 2), ]))\n",
    "train_data = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_image = next(iter(train_data))[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a very simple model to make sure it works\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, num_of_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(num_of_channels, 32, 3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, num_of_channels, 2, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape (batch_size, num_channels, height, width)\n",
    "        encode = self.encoder(x)\n",
    "        decode = self.decoder(encode)\n",
    "        \n",
    "        return decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 300\n",
    "\n",
    "# define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# define alphas \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform(img):\n",
    "    img = img.detach().cpu().numpy()\n",
    "    img = img.squeeze()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward diffusion (using the nice property)\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fotis\\Programming\\Stable diffusion\\testing.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# timesteps = [torch.tensor([i]) for i in range(0, 100, 10)]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39;49m(timesteps)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   plt\u001b[39m.\u001b[39msubplot(\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m, i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m   t \u001b[39m=\u001b[39m timesteps[i]\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_noisy_image(x_start, t):\n",
    "  # add noise\n",
    "  x_noisy = q_sample(x_start, t=t)\n",
    "  # turn back into PIL image\n",
    "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "\n",
    "  return noisy_image\n",
    "\n",
    "\n",
    "# timesteps = [torch.tensor([i]) for i in range(0, 100, 10)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(1, len(timesteps)):\n",
    "  plt.subplot(3,4, i)\n",
    "  t = timesteps[i]\n",
    "  plt.imshow(get_noisy_image(test_image, t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(denoise_model, x_start, t):\n",
    "    x_noisy = q_sample(x_start, t=t)\n",
    "    x_denoised = denoise_model(x_noisy)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = F.mse_loss(x_denoised, x_start)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Equation 11 in the paper\n",
    "    # Use our model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        # Algorithm 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
    "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "        imgs.append(img.cpu().numpy())\n",
    "    return imgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 0.941578209400177\n",
      "Loss: 0.8964121341705322\n",
      "Loss: 0.8738733530044556\n",
      "Loss: 0.8792153000831604\n",
      "Epoch: 1\n",
      "Loss: 0.8701121807098389\n",
      "Loss: 0.85712069272995\n",
      "Loss: 0.8690035939216614\n",
      "Loss: 0.8743314743041992\n",
      "Epoch: 2\n",
      "Loss: 0.892378568649292\n",
      "Loss: 0.871153712272644\n",
      "Loss: 0.876585066318512\n",
      "Loss: 0.8741531372070312\n",
      "Epoch: 3\n",
      "Loss: 0.8737866282463074\n",
      "Loss: 0.8548275828361511\n",
      "Loss: 0.8676373958587646\n",
      "Loss: 0.8890323042869568\n",
      "Epoch: 4\n",
      "Loss: 0.8713604807853699\n",
      "Loss: 0.8846539855003357\n",
      "Loss: 0.8706080317497253\n",
      "Loss: 0.8575968146324158\n",
      "Epoch: 5\n",
      "Loss: 0.8699927926063538\n",
      "Loss: 0.8569199442863464\n",
      "Loss: 0.8727914690971375\n",
      "Loss: 0.873970627784729\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 6\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for step, batch in enumerate(train_data):\n",
    "      optimizer.zero_grad()\n",
    "      batch_size = batch[0].shape[0]\n",
    "      batch = batch[0]\n",
    "\n",
    "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "      t = torch.randint(0, timesteps, (batch_size,)).long()\n",
    "\n",
    "      lossval = loss(model, batch, t)\n",
    "\n",
    "      if step % 500 == 0:\n",
    "        print(\"Loss:\", lossval.item())\n",
    "\n",
    "\n",
    "      lossval.backward()\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fotis\\Programming\\Stable diffusion\\testing.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m samples \u001b[39m=\u001b[39m sample(model, image_size\u001b[39m=\u001b[39;49m\u001b[39m28\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, channels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# show a random one\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m random_index \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fotis\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mc:\\Users\\fotis\\Programming\\Stable diffusion\\testing.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(model, image_size, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m p_sample_loop(model, shape\u001b[39m=\u001b[39;49m(batch_size, channels, image_size, image_size))\n",
      "File \u001b[1;32mc:\\Users\\fotis\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mc:\\Users\\fotis\\Programming\\Stable diffusion\\testing.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m imgs \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, timesteps)), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msampling loop time step\u001b[39m\u001b[39m'\u001b[39m, total\u001b[39m=\u001b[39mtimesteps):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     img \u001b[39m=\u001b[39m p_sample(model, img, torch\u001b[39m.\u001b[39;49mfull((b,), i, device\u001b[39m=\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong), i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     imgs\u001b[39m.\u001b[39mappend(img\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mreturn\u001b[39;00m imgs\n",
      "File \u001b[1;32mc:\\Users\\fotis\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mc:\\Users\\fotis\\Programming\\Stable diffusion\\testing.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m sqrt_recip_alphas_t \u001b[39m=\u001b[39m extract(sqrt_recip_alphas, t, x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Equation 11 in the paper\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Use our model (noise predictor) to predict the mean\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model_mean \u001b[39m=\u001b[39m sqrt_recip_alphas_t \u001b[39m*\u001b[39m (\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m-\u001b[39m betas_t \u001b[39m*\u001b[39m model(x, t) \u001b[39m/\u001b[39m sqrt_one_minus_alphas_cumprod_t\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m t_index \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fotis/Programming/Stable%20diffusion/testing.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model_mean\n",
      "File \u001b[1;32mc:\\Users\\fotis\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fotis\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "samples = sample(model, image_size=28, batch_size=64, channels=1)\n",
    "\n",
    "# show a random one\n",
    "random_index = 5\n",
    "plt.imshow(samples[-1][random_index].reshape(28, 28, 1), cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
